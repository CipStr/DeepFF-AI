{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Project: \n",
    "\n",
    "A study on reinforcement learning for the game of Super Mario Bros on the NES. \n",
    "\n",
    "By: Stricescu Razvan Ciprian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the environment based on the version of gym as the gym_super_mario_bros library is not compatible with the latest version of gym and requires apy compatibility.\n",
    "\n",
    "The main difference is in the step function which returns **{next state, reward, done, trunc, info}** instead of **{next state, reward, done, trunc, info}**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "\n",
    "env_name = env.spec.id # will be used to save files later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying.\n",
    "\n",
    "The reward function is defined as follows:\n",
    "- v: the difference in agent x values between states.\n",
    "- c: the difference in the game clock between frames.\n",
    "- d: a death penalty that penalizes the agent for dying in a state.\n",
    "\n",
    "Thus the reward function is defined as: **v + c + d**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info returned by the step function is a dictionary containing the following information:\n",
    "\n",
    "|   coins  |             number of coins             |\n",
    "|:--------:|:---------------------------------------:|\n",
    "| flag_get |      whether mario got to the flag      |\n",
    "|   life   |           number of lifes left          |\n",
    "|   score  |               total score               |\n",
    "|   stage  |            level in the world           |\n",
    "|  status  | mario's status (small, large, fireball) |\n",
    "|   time   |      time left on the in-game clock     |\n",
    "|   world  |              current world              |\n",
    "|   x_pos  |           horizontal position           |\n",
    "|   y_pos  |            vertical position            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "\n",
    "In order to optimize the training process, the following preprocessing steps were taken as environment wrappers:\n",
    "\n",
    "- **Skip Frames**: The environment is wrapped in a skip frame wrapper that skips a number of frames and returns the last frame as the state. This is done to reduce the number of frames the agent has to process and to speed up the training process.\n",
    "\n",
    "- **Gray Scale**: The environment is wrapped in a gray scale wrapper that converts the state to gray scale. This is done to reduce the number of channels the agent has to process and to speed up the training process.\n",
    "\n",
    "- **Resize**: The environment is wrapped in a resize wrapper that resizes the state to a smaller size. This is done to reduce the number of pixels the agent has to process and to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the wrappers to the environment and also define the movement space (the actions the agent can take):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True) # Version dependent as the step API changed\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "\n",
    "# Define movement space \n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and definition:\n",
    "\n",
    "The CNN architecure is quite small and simple as the agent only needs to learn the basic features of the game, but in order to experiment with different architectures I have defined three different models:\n",
    "\n",
    "- **Model 1**: A simple CNN with 3 convolutional layers and 2 fully connected layers.\n",
    "- **Model 2**: Like Model 1, but with Max Pooling layers after each convolutional layer.\n",
    "- **Model 3**: Like Model 2, but with Batch Normalization layers after each convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online network: The online network is the network that is used to make predictions and is updated at each step.\n",
    "\n",
    "Target network: The target network is the network that is used to calculate the target values and is less frequently updated. The idea is to reduce the correlation between the target Q-values and the Q-values predicted by the online network, which can lead to more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "           #nn.BatchNorm2d(32),\n",
    "           #nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "           #nn.BatchNorm2d(64),\n",
    "           #nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "           #nn.BatchNorm2d(64),\n",
    "           #nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent definition:\n",
    "\n",
    "Exploration is done using an epsilon greedy policy with a linear decay. \n",
    "\n",
    "The exploration memory is implemented using LazyMemmapStorage from the torrchrl library which is a memory wrapped storage for tensors. The replay buffer is implemented as a TensorDict with the following keys: **state, next_state, action, reward, done**.\n",
    "\n",
    "For the loss function one close to the Huber loss is used, but with a delta value. The loss function is defined as follows:\n",
    "\n",
    "**huber(x,y)/beta** (The Huber loss is defined as a piecewise function that behaves like the mean squared error loss for small errors and like the mean absolute error loss for large errors)\n",
    "\n",
    "It is less sensitive to outliers than the MSE loss and it can also prevent exploding gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim # state dimension which is (4, 84, 84)\n",
    "        self.action_dim = action_dim # action dimension which depends on the environment movement space\n",
    "        self.save_dir = save_dir # directory to save models and plots\n",
    "\n",
    "        # GPU if available\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Agent's DNN to predict the most optimal action\n",
    "        self.net = DQNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        # exploration hyperparameters\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving network weights\n",
    "\n",
    "        # Experience replay memory\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32 # no. of samples for replay\n",
    "\n",
    "        # Q-learning hyperparameters\n",
    "        self.gamma = 0.9  # discount factor for future rewards\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025) # learning rate is really low as we are learning from noisy data\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss() # reduction by default is mean\n",
    "\n",
    "        # Training parameters\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Agent will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "    \n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "    \n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns Q(s,a) - the model computes Q(s), then we select the columns of the taken actions.\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        action (``int``)\n",
    "        Outputs:\n",
    "        ``Q`` (``torch.tensor``): Q-value of the state-action pair\n",
    "        \"\"\"\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "\n",
    "    @torch.no_grad() # very important, as we do not want to update the target network\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Returns R_t + gamma * max_a(Q_target(s_t+1, a)) predicted by the target network.\n",
    "        Inputs:\n",
    "        reward (``torch.tensor``): reward of the state-action pair\n",
    "        next_state (``LazyFrame``),\n",
    "        done (``bool``)\n",
    "        Outputs:\n",
    "        ``Q`` (``torch.tensor``): Q-value of the state-action pair\n",
    "        \"\"\"\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "    \n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        \"\"\"\n",
    "        Updates the weights of Q_online network using the td_estimate and td_target\n",
    "        \"\"\"\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        \"\"\"\n",
    "        Copies the weights from Q_online to Q_target\n",
    "        \"\"\"\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves the model at self.save_dir with filename\n",
    "        {env_name}_net_{curr_step}.chkpt\n",
    "        \"\"\"\n",
    "        save_path = (\n",
    "            self.save_dir / f\"{env_name}_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"{env_name}_net saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Samples a batch from memory and updates Q_online and Q_target\n",
    "        \"\"\"\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger class:\n",
    "\n",
    "The logger class is used to log the training process and to save the model and the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        # Update metrics of current episode\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        \"Reset episode metrics\"\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        # Save metrics from current episode to history\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        # Save metrics\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "        \n",
    "        # Plot metrics\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:\n",
    "\n",
    "The training process is done in episodes, with each one being composed by a number of steps. At each step the agent takes an action, the environment returns the next state, the reward, whether the episode is done and some info about the environment then the agent then stores the transition in the replay buffer and samples a batch of transitions from it to learn from and update the loss function. \n",
    "\n",
    "The agent then logs the training process and saves the model and the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalit√† del thread dopo averla impostata\n",
      "  warnings.warn(str(err))\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperMarioBros-1-1-v0_net saved to checkpoints\\2024-01-09T16-19-58\\SuperMarioBros-1-1-v0_net_0.chkpt at step 157\n",
      "Episode 0 - Step 157 - Epsilon 0.9999607507653596 - Mean Reward 636.0 - Mean Length 157.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.914 - Time 2024-01-09T16:20:03\n",
      "Episode 20 - Step 4732 - Epsilon 0.9988176993207842 - Mean Reward 611.048 - Mean Length 225.333 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 104.378 - Time 2024-01-09T16:21:48\n",
      "Episode 39 - Step 8638 - Epsilon 0.9978428297729215 - Mean Reward 586.525 - Mean Length 215.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 88.971 - Time 2024-01-09T16:23:17\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "# Creating directories to save models and log files\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "# Create agent and logger\n",
    "agent = Agent(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "# Main training loop\n",
    "episodes = 40\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Main training loop for single episode\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        agent.save()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalit√† del thread dopo averla impostata\n",
      "  warnings.warn(str(err))\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint from checkpoint_zelda folder and render\n",
    "save_dir = Path(\"checkpoints\")\n",
    "#save_dir.mkdir(parents=True)\n",
    "\n",
    "agent = Agent(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "checkpoint = save_dir / \"300_ep/SuperMarioBros-1-1-v0_net_0.chkpt\"\n",
    "agent.net.load_state_dict(torch.load(checkpoint)[\"model\"])\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "for i in range(5000):\n",
    "    env.render()\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, trunc, info = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        env.reset()\n",
    "    time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
