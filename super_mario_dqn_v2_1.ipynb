{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuuIdydJ3bin"
      },
      "source": [
        "## Deep Learning Project:\n",
        "\n",
        "A study on reinforcement learning for the game of Super Mario Bros and Zelda on the NES.\n",
        "\n",
        "By: Stricescu Razvan Ciprian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mIz9JWw3bip"
      },
      "source": [
        "### Libraries used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9m4RHN9e3bip"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "from gym import spaces\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY, COMPLEX_MOVEMENT\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "\n",
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5wnjGRT3biq"
      },
      "source": [
        "Here I define the environment based on the version of gym as the gym_super_mario_bros library is not compatible with the latest version of gym and requires apy compatibility.\n",
        "\n",
        "The main difference is in the step function which returns **{next state, reward, done, trunc, info}** instead of **{next state, reward, done, trunc, info}**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cso8gy1c3biq",
        "outputId": "4c15e798-7afb-420f-fc85-b1478bb330c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = 'human' ,apply_api_compatibility=True)\n",
        "\n",
        "env_name = env.spec.id # will be used to save files later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkDNfUxC3biq"
      },
      "source": [
        "The reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying.\n",
        "\n",
        "The reward function is defined as follows:\n",
        "- v: the difference in agent x values between states.\n",
        "- c: the difference in the game clock between frames.\n",
        "- d: a death penalty that penalizes the agent for dying in a state.\n",
        "\n",
        "Thus the reward function is defined as: **v + c + d**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsRiIILD3bir"
      },
      "source": [
        "The info returned by the step function is a dictionary containing the following information:\n",
        "\n",
        "|   coins  |             number of coins             |\n",
        "|:--------:|:---------------------------------------:|\n",
        "| flag_get |      whether mario got to the flag      |\n",
        "|   life   |           number of lifes left          |\n",
        "|   score  |               total score               |\n",
        "|   stage  |            level in the world           |\n",
        "|  status  | mario's status (small, large, fireball) |\n",
        "|   time   |      time left on the in-game clock     |\n",
        "|   world  |              current world              |\n",
        "|   x_pos  |           horizontal position           |\n",
        "|   y_pos  |            vertical position            |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlQCkmir3bir"
      },
      "source": [
        "### Preprocessing:\n",
        "\n",
        "In order to optimize the training process, the following preprocessing steps were taken as environment wrappers:\n",
        "\n",
        "- **Skip Frames**: The environment is wrapped in a skip frame wrapper that skips a number of frames and returns the last frame as the state. This is done to reduce the number of frames the agent has to process and to speed up the training process.\n",
        "\n",
        "- **Gray Scale**: The environment is wrapped in a gray scale wrapper that converts the state to gray scale. This is done to reduce the number of channels the agent has to process and to speed up the training process.\n",
        "\n",
        "- **Resize**: The environment is wrapped in a resize wrapper that resizes the state to a smaller size. This is done to reduce the number of pixels the agent has to process and to speed up the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9cksmJYF3bir"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, trunc, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, trunc, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "class EpisodicLifeMario(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, trunc, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = info[\"life\"]\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so it's important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return  obs, reward, done, trunc, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _, info= self.env.step(0)\n",
        "            self.lives = info[\"life\"]\n",
        "        return obs\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        # assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = random.randint(\n",
        "                1, self.noop_max + 1\n",
        "            )  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ , _= self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJMwF1fP3bir"
      },
      "source": [
        "Now we apply the wrappers to the environment and also define the movement space (the actions the agent can take):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0er4A3PV3bis",
        "outputId": "4ddba2e2-7a82-4fcb-8fea-416b85e321b5"
      },
      "outputs": [],
      "source": [
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "# env = NoopResetEnv(env, noop_max=30)\n",
        "if \"SuperMarioBros\" in env_name:\n",
        "    env = EpisodicLifeMario(env)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True) # Version dependent as the step API changed\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "\n",
        "# Define movement space\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]]) #[\"NOOP\"]\n",
        "#env = JoypadSpace(env, COMPLEX_MOVEMENT) #[\"NOOP\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xVkNuuR3bis"
      },
      "source": [
        "### Model Architecture and definition:\n",
        "\n",
        "The CNN architecure is quite small and simple as the agent only needs to learn the basic features of the game, but in order to experiment with different architectures I have defined three different models:\n",
        "\n",
        "- **Model 1**: A simple CNN with 3 convolutional layers and 2 fully connected layers.\n",
        "- **Model 2**: Like Model 1, but with Max Pooling layers after each convolutional layer.\n",
        "- **Model 3**: Like Model 2, but with Batch Normalization layers after each convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-surhauN3bis"
      },
      "source": [
        "Online network: The online network is the network that is used to make predictions and is updated at each step.\n",
        "\n",
        "Target network: The target network is the network that is used to calculate the target values and is less frequently updated. The idea is to reduce the correlation between the target Q-values and the Q-values predicted by the online network, which can lead to more stable training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mewmsbl33bis"
      },
      "outputs": [],
      "source": [
        "class DQNet(nn.Module):\n",
        "    \"\"\"Deep Q-network with target network\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = self.__build_cnn(c, output_dim)\n",
        "\n",
        "        self.target = self.__build_cnn(c, output_dim)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)\n",
        "\n",
        "    def __build_cnn(self, c, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "           #nn.BatchNorm2d(32),\n",
        "           #nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "           #nn.BatchNorm2d(64),\n",
        "           #nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            #nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "           #nn.BatchNorm2d(64),\n",
        "           #nn.MaxPool2d(kernel_size=2),\n",
        "           #nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(20736, 3136),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8V2zDgP3bis"
      },
      "source": [
        "### Agent definition:\n",
        "\n",
        "Exploration is done using an epsilon greedy policy with a linear decay.\n",
        "\n",
        "The exploration memory is implemented using LazyMemmapStorage from the torrchrl library which is a memory wrapped storage for tensors. The replay buffer is implemented as a TensorDict with the following keys: **state, next_state, action, reward, done**.\n",
        "\n",
        "For the loss function one close to the Huber loss is used, but with a delta value. The loss function is defined as follows:\n",
        "\n",
        "**huber(x,y)/beta** (The Huber loss is defined as a piecewise function that behaves like the mean squared error loss for small errors and like the mean absolute error loss for large errors)\n",
        "\n",
        "It is less sensitive to outliers than the MSE loss and it can also prevent exploding gradient problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QFx88NG53bis"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim # state dimension which is (4, 84, 84)\n",
        "        self.action_dim = action_dim # action dimension which depends on the environment movement space\n",
        "        self.save_dir = save_dir # directory to save models and plots\n",
        "\n",
        "        # GPU if available\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Agent's DNN to predict the most optimal action\n",
        "        self.net = DQNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        # exploration hyperparameters\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving network weights\n",
        "\n",
        "        # Experience replay memory\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(50000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 256 # no. of samples for replay\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.gamma = 0.9  # discount factor for future rewards\n",
        "\n",
        "        # Optimizer and loss function\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025) # learning rate is really low as we are learning from noisy data\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss() # reduction by default is mean\n",
        "\n",
        "        # Training parameters\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    ``action_idx`` (``int``): An integer representing which action Agent will perform\n",
        "    \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n",
        "\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns Q(s,a) - the model computes Q(s), then we select the columns of the taken actions.\n",
        "        Inputs:\n",
        "        state (``LazyFrame``),\n",
        "        action (``int``)\n",
        "        Outputs:\n",
        "        ``Q`` (``torch.tensor``): Q-value of the state-action pair\n",
        "        \"\"\"\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "\n",
        "    @torch.no_grad() # very important, as we do not want to update the target network\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Returns R_t + gamma * max_a(Q_target(s_t+1, a)) predicted by the target network.\n",
        "        Inputs:\n",
        "        reward (``torch.tensor``): reward of the state-action pair\n",
        "        next_state (``LazyFrame``),\n",
        "        done (``bool``)\n",
        "        Outputs:\n",
        "        ``Q`` (``torch.tensor``): Q-value of the state-action pair\n",
        "        \"\"\"\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        \"\"\"\n",
        "        Updates the weights of Q_online network using the td_estimate and td_target\n",
        "        \"\"\"\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        \"\"\"\n",
        "        Copies the weights from Q_online to Q_target\n",
        "        \"\"\"\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"\n",
        "        Saves the model at self.save_dir with filename\n",
        "        {env_name}_net_{curr_step}.chkpt\n",
        "        \"\"\"\n",
        "        save_path = (\n",
        "            self.save_dir / f\"{env_name}_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"{env_name}_net saved to {save_path} at step {self.curr_step}\")\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Samples a batch from memory and updates Q_online and Q_target\n",
        "        \"\"\"\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnhklpMC8_LC"
      },
      "source": [
        "### Logger class:\n",
        "\n",
        "The logger class is used to log the training process and to save the model and the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gjzw-tZX3biu"
      },
      "outputs": [],
      "source": [
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        # Update metrics of current episode\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        \"Reset episode metrics\"\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        # Save metrics from current episode to history\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        # Save metrics\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Plot metrics\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63y15Sf9CBf"
      },
      "source": [
        "## Training:\n",
        "\n",
        "The training process is done in episodes, with each one being composed by a number of steps. At each step the agent takes an action, the environment returns the next state, the reward, whether the episode is done and some info about the environment then the agent then stores the transition in the replay buffer and samples a batch of transitions from it to learn from and update the loss function.\n",
        "\n",
        "The agent then logs the training process and saves the model and the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "LskZyp7g3biu",
        "outputId": "27825ffc-450c-4ee6-b5b1-28b126f8ab18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python3.10\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalitÃ  del thread dopo averla impostata\n",
            "  warnings.warn(str(err))\n",
            "d:\\Python3.10\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SuperMarioBros-1-1-v0_net saved to checkpoints\\2024-01-10T18-25-05\\SuperMarioBros-1-1-v0_net_0.chkpt at step 40\n",
            "Episode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.295 - Time 2024-01-10T18:25:10\n",
            "Episode 20 - Step 5210 - Epsilon 0.9986983477221798 - Mean Reward 641.667 - Mean Length 248.095 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 93.387 - Time 2024-01-10T18:26:43\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m agent\u001b[38;5;241m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[0;32m     35\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_step(reward, loss, q)\n",
            "Cell \u001b[1;32mIn[6], line 188\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m td_est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_estimate(state, action)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Get TD Target\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m td_tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtd_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Backpropagate loss through Q_online\u001b[39;00m\n\u001b[0;32m    191\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_Q_online(td_est, td_tgt)\n",
            "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[6], line 133\u001b[0m, in \u001b[0;36mAgent.td_target\u001b[1;34m(self, reward, next_state, done)\u001b[0m\n\u001b[0;32m    129\u001b[0m best_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_state_Q, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    130\u001b[0m next_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(next_state, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\n\u001b[0;32m    131\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size), best_action\n\u001b[0;32m    132\u001b[0m ]\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (reward \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_Q)\u001b[38;5;241m.\u001b[39mfloat()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GPU if available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "# Creating directories to save models and log files\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "# Create agent and logger\n",
        "agent = Agent(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "# Main training loop\n",
        "episodes = 100\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "\n",
        "    # Main training loop for single episode\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        agent.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = agent.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 100 == 0:\n",
        "        agent.save()\n",
        "\n",
        "    if (e % 20 == 0) or (e == episodes - 1):\n",
        "        logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
