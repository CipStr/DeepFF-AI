{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install https://github.com/chovanecm/python-genetic-algorithm/archive/master.zip#egg=mchgenalg\n",
    "#%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gym \n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters, I use a Genetic Algorithm to find the best parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hyperparams = {\n",
    "    'batch_size': 250,\n",
    "    'beta': 0.2,\n",
    "    'lambda': 0.1,\n",
    "    'eta': 1.0,\n",
    "    'gamma': 0.2,\n",
    "    'max_episode_length': 200,\n",
    "    'min_progress': 15,\n",
    "    'action_repeats': 6,\n",
    "    'frames_per_state': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'skip_frames': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, new_size=(42,42), to_gray=True):\n",
    "    if to_gray:\n",
    "        return resize(frame, new_size, anti_aliasing=True).max(axis=2)\n",
    "    else:\n",
    "        return resize(frame, new_size, anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_state(state):\n",
    "    return torch.from_numpy(preprocess_frame(state, to_gray=True)).float().unsqueeze(0)\n",
    "\n",
    "def prepare_multi_states(state1, state2):\n",
    "    state1 = state1.clone()\n",
    "    temp = torch.from_numpy(preprocess_frame(state2, to_gray=True)).float()\n",
    "    state1[0][0] = state1[0][1]\n",
    "    state1[0][1] = state1[0][2]\n",
    "    state1[0][2] = temp\n",
    "    return state1\n",
    "\n",
    "def prepare_initial_state(state, N=4):\n",
    "    state_ = torch.from_numpy(preprocess_frame(state, to_gray=True)).float()\n",
    "    tmp = state_.repeat((N, 1, 1))\n",
    "    return tmp.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy definition\n",
    "\n",
    "def policy(qvalues, eps=None):\n",
    "    if eps is not None:\n",
    "        if torch.rand(1) < eps:\n",
    "            return torch.randint(low=0, high=qvalues.shape[1], size=(1,))\n",
    "        else:\n",
    "            return torch.argmax(qvalues)\n",
    "    else:\n",
    "        return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay memory in order to sample mini batches of experiences for training\n",
    "from random import shuffle\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, N=500, batch_size=100):\n",
    "        self.N = N\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def add_memory(self, state1, action, reward, state2):\n",
    "        self.counter += 1\n",
    "        if self.counter % self.N == 0:\n",
    "            self.shuffle_memory()\n",
    "        if(len(self.memory) < self.N):\n",
    "            self.memory.append((state1, action, reward, state2))\n",
    "        else:\n",
    "            rand_idx = np.random.randint(0, self.N - 1)\n",
    "            self.memory[rand_idx] = (state1, action, reward, state2)\n",
    "\n",
    "    def shuffle_memory(self):\n",
    "        shuffle(self.memory)\n",
    "\n",
    "    def get_batch(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            batch_size = len(self.memory)\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        if len(self.memory) < 1:\n",
    "            print(\"Error: Memory is empty\")\n",
    "            return None\n",
    "        \n",
    "        ind = np.random.choice(np.arange(len(self.memory)), batch_size, replace=False)\n",
    "        batch = [self.memory[i] for i in ind]\n",
    "        state1_batch = torch.stack([x[0].squeeze(0) for x in batch], dim=0)\n",
    "        action_batch = torch.Tensor([x[1] for x in batch]).long()\n",
    "        reward_batch = torch.Tensor([x[2] for x in batch])\n",
    "        state2_batch = torch.stack([x[3].squeeze(0) for x in batch], dim=0)\n",
    "        return state1_batch, action_batch, reward_batch, state2_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic curiosity module: 3 diverse nn networks (forward, inverse, encoder)\n",
    "\n",
    "class Phi(nn.Module): # Encoder\n",
    "    def __init__(self):\n",
    "        super(Phi, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=1)\n",
    "        return y\n",
    "    \n",
    "class Gnet(nn.Module): # Inverse model\n",
    "    def __init__(self):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(576, 256)\n",
    "        self.fc2 = nn.Linear(256, env.action_space.n)\n",
    "\n",
    "    def forward(self, state1, state2):\n",
    "        x = torch.cat((state1, state2), dim=1)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = self.fc2(y)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        return y\n",
    "    \n",
    "class Fnet(nn.Module): # Forward model\n",
    "    def __init__(self):\n",
    "        super(Fnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 256)\n",
    "        self.fc2 = nn.Linear(256, 288)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        action_ = torch.zeros((action.shape[0], env.action_space.n))\n",
    "        indices = torch.stack((torch.arange(action.shape[0]), action.squeeze()), dim=0)\n",
    "        indices = indices.tolist()\n",
    "        action_[indices] = 1\n",
    "        x = torch.cat((state, action_), dim=1)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q network\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(288, 100)\n",
    "        self.fc2 = nn.Linear(100, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=2)\n",
    "        y = y.view(y.shape[0], -1, 32)\n",
    "        y = y.flatten(start_dim=1)\n",
    "        y = F.elu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = ExperienceReplayMemory(N=1500, batch_size=hyperparams['batch_size'])\n",
    "qnet = Qnet()\n",
    "encoder = Phi()\n",
    "forward_model = Fnet()\n",
    "inverse_model = Gnet()\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "qloss = nn.MSELoss()\n",
    "all_model_params = list(qnet.parameters()) + list(encoder.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())\n",
    "optimizer = optim.Adam(all_model_params, lr=hyperparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q_loss, forward_loss, inverse_loss, beta, lambda_value):\n",
    "    loss_ = (1 - beta)*inverse_loss\n",
    "    loss_ += hyperparams['beta']*forward_loss\n",
    "    loss_ = loss_.sum() / loss_.flatten().shape[0]\n",
    "    loss_ += lambda_value*q_loss\n",
    "    return loss_\n",
    "\n",
    "def reset_env():\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    return state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICM(state1, action, state2, forward_scale = 1., inverse_scale = 1e4):\n",
    "    state1_hat = encoder(state1)\n",
    "    state2_hat = encoder(state2)\n",
    "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach())\n",
    "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    pred_action = inverse_model(state1_hat, state2_hat)\n",
    "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, action.detach().flatten()).unsqueeze(dim=1)\n",
    "    return forward_pred_err, inverse_pred_err\n",
    "\n",
    "def minibatch_train(use_explicit=True, gamma = hyperparams['gamma']):\n",
    "    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch()\n",
    "    action_batch = action_batch.view(action_batch.shape[0], 1)\n",
    "    reward_batch = reward_batch.view(reward_batch.shape[0], 1)\n",
    "    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch)\n",
    "    i_reward = (1./hyperparams['eta'])*forward_pred_err \n",
    "    reward = i_reward.detach() \n",
    "    if use_explicit:\n",
    "        reward += reward_batch\n",
    "    qvals = qnet(state2_batch)\n",
    "    reward += gamma*torch.max(qvals)\n",
    "    reward_pred = qnet(state1_batch) \n",
    "    reward_target = reward_pred.clone()\n",
    "    indices = torch.stack((torch.arange(action_batch.shape[0]), action_batch.squeeze()), dim=0)\n",
    "    indices = indices.tolist()\n",
    "    reward_target[indices] = reward.squeeze()\n",
    "    q_loss = 1e5 * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))\n",
    "    return forward_pred_err, inverse_pred_err, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lambda_value=hyperparams['lambda'], beta=hyperparams['beta'], gamma=hyperparams['gamma'], eps = 0.15):\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    eps = eps\n",
    "    losses = []\n",
    "    episode_length = 0\n",
    "    switch_to_eps_greedy = 1000\n",
    "    state_deque = deque(maxlen=hyperparams['frames_per_state'])\n",
    "    env.reset()\n",
    "    _, _, _,info_0 = env.step(0)\n",
    "    env.reset()\n",
    "    last_x_pos = info_0['x_pos']\n",
    "    e_reward = 0\n",
    "    ep_lengths = []\n",
    "    #use_explicit = False\n",
    "    for i in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        episode_length += 1\n",
    "        q_val_pred = qnet(state1)\n",
    "        if i > switch_to_eps_greedy:\n",
    "            action = int(policy(q_val_pred, eps))\n",
    "        else:\n",
    "            action = int(policy(q_val_pred))\n",
    "        for j in range(hyperparams['action_repeats']):\n",
    "            for k in range(hyperparams['skip_frames']):\n",
    "                state2, e_reward_, done, info = env.step(action)\n",
    "                e_reward += e_reward_\n",
    "                if done:\n",
    "                    state1 = reset_env()\n",
    "                    break \n",
    "            state_deque.append(prepare_state(state2))\n",
    "        state2 = torch.stack(list(state_deque), dim=1)\n",
    "        replay.add_memory(state1, action, e_reward, state2)\n",
    "        e_reward = 0\n",
    "        if episode_length > hyperparams['max_episode_length']:\n",
    "            if (info['x_pos'] - last_x_pos) < hyperparams['min_progress']:\n",
    "                done = True\n",
    "            else:\n",
    "                last_x_pos = info['x_pos']\n",
    "        if done:\n",
    "            ep_lengths.append(episode_length)\n",
    "            episode_length = 0\n",
    "            state1 = reset_env()\n",
    "            last_x_pos = info_0['x_pos']\n",
    "        else:\n",
    "            state1 = state2\n",
    "        if len(replay.memory) < hyperparams['batch_size']:\n",
    "            continue\n",
    "        forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_explicit = False, gamma=gamma)\n",
    "        loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err, lambda_value=lambda_value, beta=beta)\n",
    "        loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean())\n",
    "        losses.append(loss_list)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return ep_lengths, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_4860\\3355323674.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)\n",
      " 55%|█████▌    | 27615/50000 [3:48:07<3:04:55,  2.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\DeepFF-AI\\super_mario_evol.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ep_len, losses_plot \u001b[39m=\u001b[39mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m, lambda_value\u001b[39m=\u001b[39;49mhyperparams[\u001b[39m'\u001b[39;49m\u001b[39mlambda\u001b[39;49m\u001b[39m'\u001b[39;49m], beta\u001b[39m=\u001b[39;49mhyperparams[\u001b[39m'\u001b[39;49m\u001b[39mbeta\u001b[39;49m\u001b[39m'\u001b[39;49m], gamma\u001b[39m=\u001b[39;49mhyperparams[\u001b[39m'\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m'\u001b[39;49m], eps \u001b[39m=\u001b[39;49m \u001b[39m0.15\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m losses_q \u001b[39m=\u001b[39m [x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m losses_plot]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m losses_f \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m losses_plot]\n",
      "\u001b[1;32md:\\DeepFF-AI\\super_mario_evol.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(replay\u001b[39m.\u001b[39mmemory) \u001b[39m<\u001b[39m hyperparams[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m forward_pred_err, inverse_pred_err, q_loss \u001b[39m=\u001b[39m minibatch_train(use_explicit \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, gamma\u001b[39m=\u001b[39;49mgamma)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(q_loss, forward_pred_err, inverse_pred_err, lambda_value\u001b[39m=\u001b[39mlambda_value, beta\u001b[39m=\u001b[39mbeta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m loss_list \u001b[39m=\u001b[39m (q_loss\u001b[39m.\u001b[39mmean(), forward_pred_err\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mmean(), inverse_pred_err\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mmean())\n",
      "\u001b[1;32md:\\DeepFF-AI\\super_mario_evol.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m action_batch \u001b[39m=\u001b[39m action_batch\u001b[39m.\u001b[39mview(action_batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m reward_batch \u001b[39m=\u001b[39m reward_batch\u001b[39m.\u001b[39mview(reward_batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m forward_pred_err, inverse_pred_err \u001b[39m=\u001b[39m ICM(state1_batch, action_batch, state2_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m i_reward \u001b[39m=\u001b[39m (\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39mhyperparams[\u001b[39m'\u001b[39m\u001b[39meta\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m*\u001b[39mforward_pred_err \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m reward \u001b[39m=\u001b[39m i_reward\u001b[39m.\u001b[39mdetach() \n",
      "\u001b[1;32md:\\DeepFF-AI\\super_mario_evol.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mICM\u001b[39m(state1, action, state2, forward_scale \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m, inverse_scale \u001b[39m=\u001b[39m \u001b[39m1e4\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     state1_hat \u001b[39m=\u001b[39m encoder(state1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     state2_hat \u001b[39m=\u001b[39m encoder(state2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     state2_hat_pred \u001b[39m=\u001b[39m forward_model(state1_hat\u001b[39m.\u001b[39mdetach(), action\u001b[39m.\u001b[39mdetach())\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\DeepFF-AI\\super_mario_evol.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mnormalize(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     y \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DeepFF-AI/super_mario_evol.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     y \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(y))\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torch\\nn\\functional.py:4719\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[0;32m   4717\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(normalize, (\u001b[39minput\u001b[39m, out), \u001b[39minput\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, eps\u001b[39m=\u001b[39meps, out\u001b[39m=\u001b[39mout)\n\u001b[0;32m   4718\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4719\u001b[0m     denom \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(p, dim, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mclamp_min(eps)\u001b[39m.\u001b[39mexpand_as(\u001b[39minput\u001b[39m)\n\u001b[0;32m   4720\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39m/\u001b[39m denom\n\u001b[0;32m   4721\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torch\\_tensor.py:708\u001b[0m, in \u001b[0;36mTensor.norm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    705\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    706\u001b[0m         Tensor\u001b[39m.\u001b[39mnorm, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, keepdim\u001b[39m=\u001b[39mkeepdim, dtype\u001b[39m=\u001b[39mdtype\n\u001b[0;32m    707\u001b[0m     )\n\u001b[1;32m--> 708\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mnorm(\u001b[39mself\u001b[39;49m, p, dim, keepdim, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torch\\functional.py:1611\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1609\u001b[0m _p \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39mif\u001b[39;00m p \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m p\n\u001b[0;32m   1610\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1611\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mvector_norm(\u001b[39minput\u001b[39;49m, _p, _dim, keepdim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1612\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1613\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mvector_norm(\u001b[39minput\u001b[39m, _p, _dim, keepdim, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ep_len, losses_plot =train(epochs=50000, lambda_value=hyperparams['lambda'], beta=hyperparams['beta'], gamma=hyperparams['gamma'], eps = 0.15)\n",
    "losses_q = [x[0].detach().numpy() for x in losses_plot]\n",
    "losses_f = [x[1].detach().numpy() for x in losses_plot]\n",
    "losses_i = [x[2].detach().numpy() for x in losses_plot]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.log(losses_q), label='Q loss')\n",
    "plt.plot(np.log(losses_f), label='Forward loss')\n",
    "plt.plot(np.log(losses_i), label='Inverse loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalità del thread dopo averla impostata\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "eps = 0.1\n",
    "done = True\n",
    "state_deque = deque(maxlen=hyperparams['frames_per_state'])\n",
    "for step in range(5000):\n",
    "    if done: \n",
    "        env.reset()\n",
    "        state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    q_val_pred = qnet(state1)\n",
    "    action = int(policy(q_val_pred, eps))\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    state2 = prepare_multi_states(state1, state2)\n",
    "    state1 = state2\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genetic algorithm for parameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
