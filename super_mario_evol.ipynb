{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install https://github.com/chovanecm/python-genetic-algorithm/archive/master.zip#egg=mchgenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gym \n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters, I use a Genetic Algorithm to find the best parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hyperparams = {\n",
    "    'batch_size': 150,\n",
    "    'beta': 0.2,\n",
    "    'lambda': 0.1,\n",
    "    'eta': 1.0,\n",
    "    'gamma': 0.2,\n",
    "    'max_episode_length': 200,\n",
    "    'min_progress': 15,\n",
    "    'action_repeats': 6,\n",
    "    'frames_per_state': 3,\n",
    "    'learning_rate': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, new_size=(42,42), to_gray=True):\n",
    "    if to_gray:\n",
    "        return resize(frame, new_size, anti_aliasing=True).max(axis=2)\n",
    "    else:\n",
    "        return resize(frame, new_size, anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_state(state):\n",
    "    return torch.from_numpy(preprocess_frame(state, to_gray=True)).float().unsqueeze(0)\n",
    "\n",
    "def prepare_multi_states(state1, state2):\n",
    "    state1 = state1.clone()\n",
    "    temp = torch.from_numpy(preprocess_frame(state2, to_gray=True)).float()\n",
    "    state1[0][0] = state1[0][1]\n",
    "    state1[0][1] = state1[0][2]\n",
    "    state1[0][2] = temp\n",
    "    return state1\n",
    "\n",
    "def prepare_initial_state(state, N=3):\n",
    "    state_ = torch.from_numpy(preprocess_frame(state, to_gray=True)).float()\n",
    "    tmp = state_.repeat((N, 1, 1))\n",
    "    return tmp.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy definition\n",
    "\n",
    "def policy(qvalues, eps=None):\n",
    "    if eps is not None:\n",
    "        if torch.rand(1) < eps:\n",
    "            return torch.randint(low=0, high=qvalues.shape[1], size=(1,))\n",
    "        else:\n",
    "            return torch.argmax(qvalues)\n",
    "    else:\n",
    "        return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay memory in order to sample mini batches of experiences for training\n",
    "from random import shuffle\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, N=500, batch_size=100):\n",
    "        self.N = N\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def add_memory(self, state1, action, reward, state2):\n",
    "        self.counter += 1\n",
    "        if self.counter % self.N == 0:\n",
    "            self.shuffle_memory()\n",
    "        if(len(self.memory) < self.N):\n",
    "            self.memory.append((state1, action, reward, state2))\n",
    "        else:\n",
    "            rand_idx = np.random.randint(0, self.N - 1)\n",
    "            self.memory[rand_idx] = (state1, action, reward, state2)\n",
    "\n",
    "    def shuffle_memory(self):\n",
    "        shuffle(self.memory)\n",
    "\n",
    "    def get_batch(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            batch_size = len(self.memory)\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        if len(self.memory) < 1:\n",
    "            print(\"Error: Memory is empty\")\n",
    "            return None\n",
    "        \n",
    "        ind = np.random.choice(np.arange(len(self.memory)), batch_size, replace=False)\n",
    "        batch = [self.memory[i] for i in ind]\n",
    "        state1_batch = torch.stack([x[0].squeeze(0) for x in batch], dim=0)\n",
    "        action_batch = torch.Tensor([x[1] for x in batch]).long()\n",
    "        reward_batch = torch.Tensor([x[2] for x in batch])\n",
    "        state2_batch = torch.stack([x[3].squeeze(0) for x in batch], dim=0)\n",
    "        return state1_batch, action_batch, reward_batch, state2_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic curiosity module: 3 diverse nn networks (forward, inverse, encoder)\n",
    "\n",
    "class Phi(nn.Module): # Encoder\n",
    "    def __init__(self):\n",
    "        super(Phi, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=1)\n",
    "        return y\n",
    "    \n",
    "class Gnet(nn.Module): # Inverse model\n",
    "    def __init__(self):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(576, 256)\n",
    "        self.fc2 = nn.Linear(256, env.action_space.n)\n",
    "\n",
    "    def forward(self, state1, state2):\n",
    "        x = torch.cat((state1, state2), dim=1)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = self.fc2(y)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        return y\n",
    "    \n",
    "class Fnet(nn.Module): # Forward model\n",
    "    def __init__(self):\n",
    "        super(Fnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 256)\n",
    "        self.fc2 = nn.Linear(256, 288)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        action_ = torch.zeros((action.shape[0], env.action_space.n))\n",
    "        indices = torch.stack((torch.arange(action.shape[0]), action.squeeze()), dim=0)\n",
    "        indices = indices.tolist()\n",
    "        action_[indices] = 1\n",
    "        x = torch.cat((state, action_), dim=1)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q network\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(288, 100)\n",
    "        self.fc2 = nn.Linear(100, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=2)\n",
    "        y = y.view(y.shape[0], -1, 32)\n",
    "        y = y.flatten(start_dim=1)\n",
    "        y = F.elu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = ExperienceReplayMemory(N=1000, batch_size=hyperparams['batch_size'])\n",
    "qnet = Qnet()\n",
    "encoder = Phi()\n",
    "forward_model = Fnet()\n",
    "inverse_model = Gnet()\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "qloss = nn.MSELoss()\n",
    "all_model_params = list(qnet.parameters()) + list(encoder.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())\n",
    "optimizer = optim.Adam(all_model_params, lr=hyperparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q_loss, forward_loss, inverse_loss, beta, lambda_value):\n",
    "    loss_ = (1 - beta)*inverse_loss\n",
    "    loss_ += hyperparams['beta']*forward_loss\n",
    "    loss_ = loss_.sum() / loss_.flatten().shape[0]\n",
    "    loss_ += lambda_value*q_loss\n",
    "    return loss_\n",
    "\n",
    "def reset_env():\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    return state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICM(state1, action, state2, forward_scale = 1., inverse_scale = 1e4):\n",
    "    state1_hat = encoder(state1)\n",
    "    state2_hat = encoder(state2)\n",
    "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach())\n",
    "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    pred_action = inverse_model(state1_hat, state2_hat)\n",
    "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, action.detach().flatten()).unsqueeze(dim=1)\n",
    "    return forward_pred_err, inverse_pred_err\n",
    "\n",
    "def minibatch_train(use_explicit=True, gamma = hyperparams['gamma']):\n",
    "    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch()\n",
    "    action_batch = action_batch.view(action_batch.shape[0], 1)\n",
    "    reward_batch = reward_batch.view(reward_batch.shape[0], 1)\n",
    "    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch)\n",
    "    i_reward = (1./hyperparams['eta'])*forward_pred_err \n",
    "    reward = i_reward.detach() \n",
    "    if use_explicit:\n",
    "        reward += reward_batch\n",
    "    qvals = qnet(state2_batch)\n",
    "    reward += gamma*torch.max(qvals)\n",
    "    reward_pred = qnet(state1_batch) \n",
    "    reward_target = reward_pred.clone()\n",
    "    indices = torch.stack((torch.arange(action_batch.shape[0]), action_batch.squeeze()), dim=0)\n",
    "    indices = indices.tolist()\n",
    "    reward_target[indices] = reward.squeeze()\n",
    "    q_loss = 1e5 * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))\n",
    "    return forward_pred_err, inverse_pred_err, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lambda_value=hyperparams['lambda'], beta=hyperparams['beta'], gamma=hyperparams['gamma'], eps = 0.15):\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    eps = eps\n",
    "    losses = []\n",
    "    episode_length = 0\n",
    "    switch_to_eps_greedy = 1000\n",
    "    state_deque = deque(maxlen=hyperparams['frames_per_state'])\n",
    "    env.reset()\n",
    "    _, _, _, info_0 = env.step(0)\n",
    "    env.reset()\n",
    "    last_x_pos = info_0['x_pos']\n",
    "    e_reward = 0\n",
    "    ep_lengths = []\n",
    "    #use_explicit = False\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        episode_length += 1\n",
    "        q_val_pred = qnet(state1)\n",
    "        if i > switch_to_eps_greedy:\n",
    "            action = int(policy(q_val_pred, eps))\n",
    "        else:\n",
    "            action = int(policy(q_val_pred))\n",
    "        for j in range(hyperparams['action_repeats']):\n",
    "            state2, e_reward_, done, info = env.step(action)\n",
    "            if done:\n",
    "                state1 = reset_env()\n",
    "                break\n",
    "            e_reward += e_reward_ \n",
    "            state_deque.append(prepare_state(state2))\n",
    "        state2 = torch.stack(list(state_deque), dim=1)\n",
    "        replay.add_memory(state1, action, e_reward, state2)\n",
    "        e_reward = 0\n",
    "        if episode_length > hyperparams['max_episode_length']:\n",
    "            if (info['x_pos'] - last_x_pos) < hyperparams['min_progress']:\n",
    "                done = True\n",
    "            else:\n",
    "                last_x_pos = info['x_pos']\n",
    "        if done:\n",
    "            ep_lengths.append(episode_length)\n",
    "            episode_length = 0\n",
    "            state1 = reset_env()\n",
    "            last_x_pos = info_0['x_pos']\n",
    "        else:\n",
    "            state1 = state2\n",
    "        if len(replay.memory) < hyperparams['batch_size']:\n",
    "            continue\n",
    "        forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_explicit = False, gamma=gamma)\n",
    "        loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err, lambda_value=lambda_value, beta=beta)\n",
    "        loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean())\n",
    "        losses.append(loss_list)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return ep_lengths, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genetic algorithm for parameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating genome for the 1 time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_15896\\3355323674.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)\n",
      "d:\\Python3.10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "d:\\Python3.10\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating genome for the 2 time\n",
      "evaluating genome for the 3 time\n",
      "evaluating genome for the 4 time\n",
      "evaluating genome for the 5 time\n",
      "evaluating genome for the 6 time\n",
      "evaluating genome for the 7 time\n",
      "evaluating genome for the 8 time\n",
      "evaluating genome for the 9 time\n",
      "evaluating genome for the 10 time\n",
      "evaluating genome for the 11 time\n",
      "evaluating genome for the 12 time\n",
      "evaluating genome for the 13 time\n",
      "evaluating genome for the 14 time\n",
      "evaluating genome for the 15 time\n",
      "evaluating genome for the 16 time\n",
      "evaluating genome for the 17 time\n",
      "evaluating genome for the 18 time\n",
      "evaluating genome for the 19 time\n",
      "evaluating genome for the 20 time\n",
      "evaluating genome for the 21 time\n",
      "evaluating genome for the 22 time\n",
      "evaluating genome for the 23 time\n",
      "evaluating genome for the 24 time\n",
      "evaluating genome for the 25 time\n",
      "evaluating genome for the 26 time\n",
      "evaluating genome for the 27 time\n",
      "evaluating genome for the 28 time\n",
      "evaluating genome for the 29 time\n",
      "evaluating genome for the 30 time\n",
      "evaluating genome for the 31 time\n",
      "evaluating genome for the 32 time\n",
      "evaluating genome for the 33 time\n",
      "evaluating genome for the 34 time\n",
      "evaluating genome for the 35 time\n",
      "evaluating genome for the 36 time\n",
      "evaluating genome for the 37 time\n",
      "evaluating genome for the 38 time\n",
      "evaluating genome for the 39 time\n",
      "evaluating genome for the 40 time\n",
      "evaluating genome for the 41 time\n",
      "evaluating genome for the 42 time\n",
      "evaluating genome for the 43 time\n",
      "evaluating genome for the 44 time\n",
      "evaluating genome for the 45 time\n",
      "evaluating genome for the 46 time\n",
      "evaluating genome for the 47 time\n",
      "evaluating genome for the 48 time\n",
      "evaluating genome for the 49 time\n",
      "evaluating genome for the 50 time\n",
      "evaluating genome for the 51 time\n",
      "evaluating genome for the 52 time\n",
      "evaluating genome for the 53 time\n",
      "evaluating genome for the 54 time\n",
      "evaluating genome for the 55 time\n",
      "evaluating genome for the 56 time\n",
      "evaluating genome for the 57 time\n",
      "evaluating genome for the 58 time\n",
      "evaluating genome for the 59 time\n",
      "evaluating genome for the 60 time\n",
      "evaluating genome for the 61 time\n",
      "evaluating genome for the 62 time\n",
      "evaluating genome for the 63 time\n",
      "evaluating genome for the 64 time\n",
      "evaluating genome for the 65 time\n",
      "evaluating genome for the 66 time\n",
      "evaluating genome for the 67 time\n",
      "evaluating genome for the 68 time\n",
      "evaluating genome for the 69 time\n",
      "evaluating genome for the 70 time\n",
      "evaluating genome for the 71 time\n",
      "evaluating genome for the 72 time\n",
      "evaluating genome for the 73 time\n",
      "evaluating genome for the 74 time\n",
      "evaluating genome for the 75 time\n",
      "evaluating genome for the 76 time\n",
      "evaluating genome for the 77 time\n",
      "evaluating genome for the 78 time\n",
      "evaluating genome for the 79 time\n",
      "evaluating genome for the 80 time\n",
      "evaluating genome for the 81 time\n",
      "evaluating genome for the 82 time\n",
      "evaluating genome for the 83 time\n",
      "evaluating genome for the 84 time\n",
      "evaluating genome for the 85 time\n",
      "evaluating genome for the 86 time\n",
      "evaluating genome for the 87 time\n",
      "evaluating genome for the 88 time\n",
      "evaluating genome for the 89 time\n",
      "evaluating genome for the 90 time\n",
      "evaluating genome for the 91 time\n",
      "evaluating genome for the 92 time\n",
      "evaluating genome for the 93 time\n",
      "evaluating genome for the 94 time\n",
      "evaluating genome for the 95 time\n",
      "evaluating genome for the 96 time\n",
      "evaluating genome for the 97 time\n",
      "evaluating genome for the 98 time\n",
      "evaluating genome for the 99 time\n",
      "evaluating genome for the 100 time\n",
      "Best genome: (array([ True, False, False, False,  True, False, False, False, False,\n",
      "        True, False,  True,  True,  True,  True, False, False, False,\n",
      "       False,  True,  True,  True,  True,  True, False, False,  True,\n",
      "        True,  True, False, False, False,  True, False, False,  True,\n",
      "        True, False, False,  True, False, False, False, False,  True,\n",
      "        True,  True, False,  True, False,  True,  True,  True, False,\n",
      "        True, False, False, False,  True,  True, False,  True, False,\n",
      "       False, False, False]), nan)\n"
     ]
    }
   ],
   "source": [
    "from mchgenalg import GeneticAlgorithm\n",
    "\n",
    "times_evaluated = 0\n",
    "best_epochs = -1\n",
    "\n",
    "# function that evaluates the fitness \n",
    "def fitness_function(genome):\n",
    "    global times_evaluated\n",
    "    times_evaluated += 1\n",
    "    print(\"evaluating genome for the {} time\".format(times_evaluated))\n",
    "    beta = decode_genome(genome[0:10])\n",
    "    if beta < 0:\n",
    "        beta = 0\n",
    "    elif beta > 1:\n",
    "        beta = 1\n",
    "    lambda_ = decode_genome(genome[11:21])\n",
    "    if lambda_ < 0:\n",
    "        lambda_ = 0\n",
    "    elif lambda_ > 1:\n",
    "        lambda_ = 1\n",
    "    gamma = decode_genome(genome[22:33])\n",
    "    if gamma < 0:\n",
    "        gamma = 0\n",
    "    elif gamma > 1:\n",
    "        gamma = 1\n",
    "    eps = decode_genome(genome[56:66])\n",
    "    if eps < 0:\n",
    "        eps = 0\n",
    "    elif eps > 1:\n",
    "        eps = 1\n",
    "    epochs_default = 100\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    ep_lengths, losses = train(epochs_default, lambda_value=lambda_, beta=beta, gamma=gamma, eps=eps)\n",
    "    return np.mean(ep_lengths)\n",
    "\n",
    "\n",
    "def decode_genome(genome):\n",
    "    prod = 0\n",
    "    for i,e in reversed(list(enumerate(genome))):\n",
    "        if e == False:\n",
    "            prod += 0\n",
    "        else:\n",
    "            prod += 2**abs(i-len(genome)+1)\n",
    "    return prod/1000\n",
    "\n",
    "pop_size = 100 \n",
    "genome_length = 66\n",
    "ga = GeneticAlgorithm(fitness_function)\n",
    "ga.generate_binary_population(pop_size, genome_length)\n",
    "ga.number_of_pairs = 10\n",
    "ga.mutation_rate = 0.1\n",
    "ga.selective_pressure = 1.5\n",
    "ga.mutation_rate = 0.1\n",
    "ga.allow_random_parent = True\n",
    "ga.single_point_cross_over = False\n",
    "ga.run\n",
    "best_genome = ga.get_best_genome()\n",
    "print(\"Best genome: {}\".format(best_genome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True, False, False, False,  True, False, False, False, False,\n",
       "         True, False,  True,  True,  True,  True, False, False, False,\n",
       "        False,  True,  True,  True,  True,  True, False, False,  True,\n",
       "         True,  True, False, False, False,  True, False, False,  True,\n",
       "         True, False, False,  True, False, False, False, False,  True,\n",
       "         True,  True, False,  True, False,  True,  True,  True, False,\n",
       "         True, False, False, False,  True,  True, False,  True, False,\n",
       "        False, False, False]),\n",
       " nan)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
