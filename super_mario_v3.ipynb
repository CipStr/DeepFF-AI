{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Project: \n",
    "\n",
    "A study on reinforcement learning for the game of Super Mario Bros on the NES. \n",
    "\n",
    "By: Stricescu Razvan Ciprian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
    "from gym import spaces\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY, COMPLEX_MOVEMENT\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the environment based on the version of gym as the gym_super_mario_bros library is not compatible with the latest version of gym and requires apy compatibility.\n",
    "\n",
    "The main difference is in the step function which returns **{next state, reward, done, trunc, info}** instead of **{next state, reward, done, trunc, info}**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = 'human' ,apply_api_compatibility=True)\n",
    "\n",
    "env_name = env.spec.id # will be used to save files later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying.\n",
    "\n",
    "The reward function is defined as follows:\n",
    "- v: the difference in agent x values between states.\n",
    "- c: the difference in the game clock between frames.\n",
    "- d: a death penalty that penalizes the agent for dying in a state.\n",
    "\n",
    "Thus the reward function is defined as: **v + c + d**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info returned by the step function is a dictionary containing the following information:\n",
    "\n",
    "|   coins  |             number of coins             |\n",
    "|:--------:|:---------------------------------------:|\n",
    "| flag_get |      whether mario got to the flag      |\n",
    "|   life   |           number of lifes left          |\n",
    "|   score  |               total score               |\n",
    "|   stage  |            level in the world           |\n",
    "|  status  | mario's status (small, large, fireball) |\n",
    "|   time   |      time left on the in-game clock     |\n",
    "|   world  |              current world              |\n",
    "|   x_pos  |           horizontal position           |\n",
    "|   y_pos  |            vertical position            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "\n",
    "In order to optimize the training process, the following preprocessing steps were taken as environment wrappers:\n",
    "\n",
    "- **Skip Frames**: The environment is wrapped in a skip frame wrapper that skips a number of frames and returns the last frame as the state. This is done to reduce the number of frames the agent has to process and to speed up the training process.\n",
    "\n",
    "- **Gray Scale**: The environment is wrapped in a gray scale wrapper that converts the state to gray scale. This is done to reduce the number of channels the agent has to process and to speed up the training process.\n",
    "\n",
    "- **Resize**: The environment is wrapped in a resize wrapper that resizes the state to a smaller size. This is done to reduce the number of pixels the agent has to process and to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, trunc, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunc, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "def apply_wrappers(env):\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = FrameStack(env, num_stack=4, lz4_compress=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, freeze=False):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        if freeze:\n",
    "            self._freeze()\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_layers(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def _freeze(self):\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, input_dims, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # Hyperparams\n",
    "        self.lr = 0.00025\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.99999975\n",
    "        self.batch_size = 32\n",
    "        self.sync_net_rate = 10000\n",
    "\n",
    "        # Nets\n",
    "        self.q_net = AgentNN(input_dims, num_actions)\n",
    "        self.target_net = AgentNN(input_dims, num_actions, freeze=True)\n",
    "\n",
    "        # Optimizer \n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = nn.MSELoss() # Tried Huber loss, but it didn't work well\n",
    "\n",
    "        # Replay buffer\n",
    "        storage = LazyMemmapStorage(100000)\n",
    "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(self.q_net.device)\n",
    "        return self.q_net(state).argmax().item()\n",
    "    \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(TensorDict({\n",
    "            'state': torch.tensor(np.array(state), dtype=torch.float32),\n",
    "            'action': torch.tensor(action),\n",
    "            'reward': torch.tensor(reward),\n",
    "            'next_state': torch.tensor(np.array(next_state), dtype=torch.float32),\n",
    "            'done': torch.tensor(done)\n",
    "        }, batch_size=[]))\n",
    "\n",
    "    \n",
    "    def sync_nets(self):\n",
    "        if self.learn_step_counter % self.sync_net_rate == 0 and self.learn_step_counter > 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            torch.save(self.q_net.state_dict(), \"{env_name}_q.pth\")\n",
    "            torch.save(self.target_net.state_dict(), \"{env_name}_q_target.pth\")\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.sync_nets()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        samples = self.replay_buffer.sample(self.batch_size).to(self.q_net.device)\n",
    "\n",
    "        keys = ['state', 'action', 'reward', 'next_state', 'done']\n",
    "\n",
    "        states, actions, rewards, next_states, dones = [samples[k] for k in keys]\n",
    "\n",
    "        predicted_q_values = self.q_net(states)\n",
    "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
    "\n",
    "        target_q_values = self.target_net(next_states).max(dim=1)[0]\n",
    "        target_q_values = rewards + self.gamma*target_q_values*(1-dones.float())\n",
    "\n",
    "        loss = self.loss(predicted_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "        self.decay_epsilon()\n",
    "\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.online_network.state_dict(), path)\n",
    "\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.online_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        # Update metrics of current episode\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        \"Reset episode metrics\"\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        # Save metrics from current episode to history\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        # Save metrics\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        # Plot metrics\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "\n",
    "    def start(self):\n",
    "        self.t = time.time()\n",
    "\n",
    "    def print(self, msg=''):\n",
    "        print(f\"Time taken: {msg}\", time.time() - self.t)\n",
    "\n",
    "    def get(self):\n",
    "        return time.time() - self.t\n",
    "    \n",
    "    def store(self):\n",
    "        self.times.append(time.time() - self.t)\n",
    "\n",
    "    def average(self):\n",
    "        return sum(self.times) / len(self.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.10\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalità del thread dopo averla impostata\n",
      "  warnings.warn(str(err))\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "d:\\Python3.10\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda |Episode : 10 | score : 787.200000 | epsilon : 0.9991 | buffer size : 3780 | learn step counter : 3749\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SHOULD_TRAIN:\n\u001b[0;32m     35\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     38\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_nets()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 65\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     67\u001b[0m keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     69\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m [samples[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys]\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:809\u001b[0m, in \u001b[0;36mTensorDictReplayBuffer.sample\u001b[1;34m(self, batch_size, return_info, include_info)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    802\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_info is going to be deprecated soon.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    804\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default behaviour has changed to `include_info=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    805\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto avoid bugs linked to wrongly preassigned values in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput tensordict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m     )\n\u001b[1;32m--> 809\u001b[0m data, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensorclass(data) \u001b[38;5;129;01mand\u001b[39;00m include_info \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    811\u001b[0m     is_locked \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mis_locked\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:365\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size, return_info)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size not specified. You can specify the batch_size when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructing the replay buffer, or pass it to the sample method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to the ReplayBuffer documentation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor a proper usage of the batch-size arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch:\n\u001b[1;32m--> 365\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_queue) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torchrl\\data\\replay_buffers\\utils.py:46\u001b[0m, in \u001b[0;36mpin_memory_output.<locals>.decorated_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorated_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 46\u001b[0m     output \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m     48\u001b[0m         _tuple_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:302\u001b[0m, in \u001b[0;36mReplayBuffer._sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    300\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage[index]\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, INT_CLASSES):\n\u001b[1;32m--> 302\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform):\n\u001b[0;32m    304\u001b[0m     is_td \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:745\u001b[0m, in \u001b[0;36m_collate_as_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collate_as_tensor\u001b[39m(x):\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\tensordict\\tensordict.py:1815\u001b[0m, in \u001b[0;36mTensorDictBase.as_tensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls as_tensor on all the tensors contained in the object.\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m \n\u001b[0;32m   1810\u001b[0m \u001b[38;5;124;03mThis is reserved to classes that contain exclusively MemmapTensors,\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;124;03mand will raise an exception in all other cases.\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \n\u001b[0;32m   1813\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1818\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have an \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1819\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause at least one of its tensors does not support this method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1820\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\tensordict\\tensordict.py:1668\u001b[0m, in \u001b[0;36mTensorDictBase._fast_apply\u001b[1;34m(self, fn, batch_size, device, names, inplace, *others, **constructor_kwargs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fast_apply\u001b[39m(\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1653\u001b[0m     fn: Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstructor_kwargs,\n\u001b[0;32m   1660\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A faster apply method.\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03m    This method does not run any check after performing the func. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1666\u001b[0m \n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_nest(\n\u001b[0;32m   1669\u001b[0m         fn,\n\u001b[0;32m   1670\u001b[0m         \u001b[38;5;241m*\u001b[39mothers,\n\u001b[0;32m   1671\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1672\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   1673\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m   1674\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   1675\u001b[0m         checked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstructor_kwargs,\n\u001b[0;32m   1677\u001b[0m     )\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\tensordict\\tensordict.py:1633\u001b[0m, in \u001b[0;36mTensorDictBase._apply_nest\u001b[1;34m(self, fn, batch_size, device, names, inplace, checked, *others, **constructor_kwargs)\u001b[0m\n\u001b[0;32m   1623\u001b[0m     item_trsf \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39m_apply_nest(\n\u001b[0;32m   1624\u001b[0m         fn,\n\u001b[0;32m   1625\u001b[0m         \u001b[38;5;241m*\u001b[39m_others,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstructor_kwargs,\n\u001b[0;32m   1631\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1633\u001b[0m     item_trsf \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_others\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item_trsf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1635\u001b[0m     \u001b[38;5;66;03m# if `self` is a `SubTensorDict` we want to process the input,\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;66;03m# hence we call `set` rather than `_set_str`.\u001b[39;00m\n\u001b[0;32m   1637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, SubTensorDict):\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\tensordict\\tensordict.py:1815\u001b[0m, in \u001b[0;36mTensorDictBase.as_tensor.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls as_tensor on all the tensors contained in the object.\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m \n\u001b[0;32m   1810\u001b[0m \u001b[38;5;124;03mThis is reserved to classes that contain exclusively MemmapTensors,\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;124;03mand will raise an exception in all other cases.\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \n\u001b[0;32m   1813\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1818\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have an \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1819\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause at least one of its tensors does not support this method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1820\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\tensordict\\memmap.py:759\u001b[0m, in \u001b[0;36mMemmapTensor.as_tensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;66;03m# TorchSnapshot doesn't know how to stream MemmapTensor, so we view MemmapTensor\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# as a Tensor for saving and loading purposes. This doesn't incur any copy.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index:\n\u001b[1;32m--> 759\u001b[0m     indexed_memmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemmap_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    761\u001b[0m         indexed_memmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item(_idx, indexed_memmap)\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\tensordict\\memmap.py:387\u001b[0m, in \u001b[0;36mMemmapTensor._get_item\u001b[1;34m(self, idx, memmap_array)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# wrapping list index in tuple to avoid following warning when indexing\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;66;03m# FutureWarning: Using a non-tuple sequence for multidimensional indexing\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# this will be interpreted as an array index, `arr[np.array(seq)]`, which\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# will result either in an error or a different result.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (idx,)\n\u001b[1;32m--> 387\u001b[0m memmap_array \u001b[38;5;241m=\u001b[39m \u001b[43mmemmap_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m memmap_array\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\numpy\\core\\memmap.py:335\u001b[0m, in \u001b[0;36mmemmap.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m--> 335\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(res) \u001b[38;5;129;01mis\u001b[39;00m memmap \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39m_mmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mndarray)\n",
      "File \u001b[1;32md:\\Python3.10\\lib\\site-packages\\numpy\\core\\memmap.py:289\u001b[0m, in \u001b[0;36mmemmap.__array_finalize__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_finalize__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_mmap\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mmap \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_mmap\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = apply_wrappers(env)\n",
    "NUM_EPISODES = 1000\n",
    "update_interval = 10\n",
    "CKPT_SAVE_INTERVAL = 100\n",
    "SHOULD_TRAIN = True\n",
    "\n",
    "# Creating directories to save models and log files\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "if not SHOULD_TRAIN:\n",
    "    folder_name = \"\"\n",
    "    ckpt_name = \"\"\n",
    "    agent.load_model(os.path.join(save_dir, folder_name, ckpt_name))\n",
    "    agent.epsilon = 0.2\n",
    "    agent.eps_min = 0.0\n",
    "    agent.eps_decay = 0.0\n",
    "\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "total_score = 0.0\n",
    "# open a log file\n",
    "log_file = open(os.path.join(save_dir, \"log.txt\"), \"w\")\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        total_score += reward\n",
    "        if SHOULD_TRAIN:\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "        state = next_state\n",
    "        env.render()\n",
    "    \n",
    "    if i % update_interval == 0 and i > 0:\n",
    "        print(\n",
    "                \"%s |Episode : %d | score : %f | epsilon : %.4f | buffer size : %d | learn step counter : %d\" \n",
    "                % (\n",
    "                    \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    i,\n",
    "                    total_score /update_interval, \n",
    "                    agent.epsilon,\n",
    "                    len(agent.replay_buffer),\n",
    "                    agent.learn_step_counter\n",
    "                )\n",
    "            )\n",
    "        log_file = open(os.path.join(save_dir, \"log.txt\"), \"w\")\n",
    "        log_file.write(\n",
    "                \"%s |Episode : %d | score : %f | epsilon : %.4f | buffer size : %d | learn step counter : %d\\n\" \n",
    "                % (\n",
    "                    \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    i,\n",
    "                    total_score /update_interval, \n",
    "                    agent.epsilon,\n",
    "                    len(agent.replay_buffer),\n",
    "                    agent.learn_step_counter\n",
    "                    # write log file\n",
    "                )\n",
    "            )\n",
    "        log_file.close()\n",
    "        total_score = 0.0\n",
    "    \n",
    "    if SHOULD_TRAIN and (i + 1) % CKPT_SAVE_INTERVAL == 0:\n",
    "        agent.save_model(os.path.join(save_dir, \"model_{env_name}_\" + str(i + 1) + \"_iter.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
