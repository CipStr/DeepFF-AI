{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install https://github.com/chovanecm/python-genetic-algorithm/archive/master.zip#egg=mchgenalg\n",
    "#%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gym \n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters, I use a Genetic Algorithm to find the best parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, new_size=(42,42), to_gray=True):\n",
    "    if to_gray:\n",
    "        return resize(frame, new_size, anti_aliasing=True).max(axis=2)\n",
    "    else:\n",
    "        return resize(frame, new_size, anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_state(state):\n",
    "    return torch.from_numpy(preprocess_frame(state, to_gray=True)).float().unsqueeze(0)\n",
    "\n",
    "def prepare_multi_states(state1, state2):\n",
    "    state1 = state1.clone()\n",
    "    temp = torch.from_numpy(preprocess_frame(state2, to_gray=True)).float()\n",
    "    state1[0][0] = state1[0][1]\n",
    "    state1[0][1] = state1[0][2]\n",
    "    state1[0][2] = temp\n",
    "    return state1\n",
    "\n",
    "def prepare_initial_state(state, N=4):\n",
    "    state_ = torch.from_numpy(preprocess_frame(state, to_gray=True)).float()\n",
    "    tmp = state_.repeat((N, 1, 1))\n",
    "    return tmp.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy definition\n",
    "\n",
    "def policy(qvalues, eps=None):\n",
    "    if eps is not None:\n",
    "        if torch.rand(1) < eps:\n",
    "            return torch.randint(low=0, high=qvalues.shape[1], size=(1,))\n",
    "        else:\n",
    "            return torch.argmax(qvalues)\n",
    "    else:\n",
    "        return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay memory in order to sample mini batches of experiences for training\n",
    "from random import shuffle\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, N=500, batch_size=100):\n",
    "        self.N = N\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def add_memory(self, state1, action, reward, state2):\n",
    "        self.counter += 1\n",
    "        if self.counter % self.N == 0:\n",
    "            self.shuffle_memory()\n",
    "        if(len(self.memory) < self.N):\n",
    "            self.memory.append((state1, action, reward, state2))\n",
    "        else:\n",
    "            rand_idx = np.random.randint(0, self.N - 1)\n",
    "            self.memory[rand_idx] = (state1, action, reward, state2)\n",
    "\n",
    "    def shuffle_memory(self):\n",
    "        shuffle(self.memory)\n",
    "\n",
    "    def get_batch(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            batch_size = len(self.memory)\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        if len(self.memory) < 1:\n",
    "            print(\"Error: Memory is empty\")\n",
    "            return None\n",
    "        \n",
    "        ind = np.random.choice(np.arange(len(self.memory)), batch_size, replace=False)\n",
    "        batch = [self.memory[i] for i in ind]\n",
    "        state1_batch = torch.stack([x[0].squeeze(0) for x in batch], dim=0)\n",
    "        state1_batch.to(device)\n",
    "        action_batch = torch.Tensor([x[1] for x in batch]).long()\n",
    "        action_batch.to(device)\n",
    "        reward_batch = torch.Tensor([x[2] for x in batch])\n",
    "        reward_batch.to(device)\n",
    "        state2_batch = torch.stack([x[3].squeeze(0) for x in batch], dim=0)\n",
    "        state2_batch.to(device)\n",
    "        return state1_batch, action_batch, reward_batch, state2_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic curiosity module: 3 diverse nn networks (forward, inverse, encoder)\n",
    "\n",
    "class Phi(nn.Module): # Encoder\n",
    "    def __init__(self):\n",
    "        super(Phi, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=1)\n",
    "        return y\n",
    "    \n",
    "class Gnet(nn.Module): # Inverse model\n",
    "    def __init__(self):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(576, 256)\n",
    "        self.fc2 = nn.Linear(256, env.action_space.n)\n",
    "\n",
    "    def forward(self, state1, state2):\n",
    "        x = torch.cat((state1, state2), dim=1)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = self.fc2(y)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        return y\n",
    "    \n",
    "class Fnet(nn.Module): # Forward model\n",
    "    def __init__(self):\n",
    "        super(Fnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 256)\n",
    "        self.fc2 = nn.Linear(256, 288)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        action_ = torch.zeros((action.shape[0], env.action_space.n)).cuda()\n",
    "        indices = torch.stack((torch.arange(action.shape[0]).cuda(), action.squeeze().cuda()), dim=0).cuda()\n",
    "        indices = indices.tolist()\n",
    "        action_[indices] = 1\n",
    "        x = torch.cat((state, action_), dim=1)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q network\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(288, 100)\n",
    "        self.fc2 = nn.Linear(100, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=2)\n",
    "        y = y.view(y.shape[0], -1, 32)\n",
    "        y = y.flatten(start_dim=1)\n",
    "        y = F.elu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hyperparams = {\n",
    "    'batch_size': [\n",
    "        32, 64, 128, 256, 512], # from 32 to 512\n",
    "    'beta': 0.2,\n",
    "    'lambda': 0.1,\n",
    "    'eta': 1.0,\n",
    "    'gamma': 0.2,\n",
    "    'max_episode_length': 200,\n",
    "    'min_progress': 15,\n",
    "    'action_repeats': 6,\n",
    "    'frames_per_state': 4,\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "    'skip_frames': 4\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q_loss, forward_loss, inverse_loss, beta, lambda_value):\n",
    "    loss_ = (1 - beta)*inverse_loss\n",
    "    loss_ += hyperparams['beta']*forward_loss\n",
    "    loss_ = loss_.sum() / loss_.flatten().shape[0]\n",
    "    loss_ += lambda_value*q_loss\n",
    "    return loss_\n",
    "\n",
    "def reset_env():\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    return state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Qnet()\n",
    "qnet.cuda()\n",
    "encoder = Phi()\n",
    "encoder.cuda()\n",
    "forward_model = Fnet()\n",
    "forward_model.cuda()\n",
    "inverse_model = Gnet()\n",
    "inverse_model.cuda()\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "forward_loss.cuda()\n",
    "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "inverse_loss.cuda()\n",
    "qloss = nn.MSELoss()\n",
    "qloss.cuda()\n",
    "all_model_params = list(qnet.parameters()) + list(encoder.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICM(state1, action, state2, forward_scale = 1., inverse_scale = 1e4):\n",
    "    state1_hat = encoder(state1.cuda())\n",
    "    state2_hat = encoder(state2.cuda())\n",
    "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach())\n",
    "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    pred_action = inverse_model(state1_hat, state2_hat)\n",
    "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, action.detach().flatten()).unsqueeze(dim=1)\n",
    "    return forward_pred_err, inverse_pred_err\n",
    "\n",
    "def minibatch_train(use_explicit=True, gamma = hyperparams['gamma']):\n",
    "    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch()\n",
    "    action_batch = action_batch.view(action_batch.shape[0], 1)\n",
    "    reward_batch = reward_batch.view(reward_batch.shape[0], 1)\n",
    "    forward_pred_err, inverse_pred_err = ICM(state1_batch.cuda(), action_batch.cuda(), state2_batch)\n",
    "    i_reward = (1./hyperparams['eta'])*forward_pred_err \n",
    "    reward = i_reward.detach() \n",
    "    if use_explicit:\n",
    "        reward += reward_batch\n",
    "    qvals = qnet(state2_batch.cuda())\n",
    "    reward += gamma*torch.max(qvals)\n",
    "    reward_pred = qnet(state1_batch.cuda()) \n",
    "    reward_target = reward_pred.clone()\n",
    "    indices = torch.stack((torch.arange(action_batch.shape[0]), action_batch.squeeze()), dim=0).cuda()\n",
    "    indices = indices.tolist()\n",
    "    reward_target[indices] = reward.squeeze()\n",
    "    q_loss = 1e5 * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))\n",
    "    return forward_pred_err, inverse_pred_err, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lambda_value=hyperparams['lambda'], beta=hyperparams['beta'], gamma=hyperparams['gamma'], eps = 0.15, batch_size=None):\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    state1.to(device)\n",
    "    eps = eps\n",
    "    losses = []\n",
    "    episode_length = 0\n",
    "    switch_to_eps_greedy = 1000\n",
    "    state_deque = deque(maxlen=hyperparams['frames_per_state'])\n",
    "    env.reset()\n",
    "    _, _, _,info_0 = env.step(0)\n",
    "    env.reset()\n",
    "    last_x_pos = info_0['x_pos']\n",
    "    e_reward = 0\n",
    "    ep_lengths = []\n",
    "    #use_explicit = False\n",
    "    for i in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        episode_length += 1\n",
    "        q_val_pred = qnet(state1.cuda())\n",
    "        if i > switch_to_eps_greedy:\n",
    "            action = int(policy(q_val_pred, eps))\n",
    "        else:\n",
    "            action = int(policy(q_val_pred))\n",
    "        for j in range(hyperparams['action_repeats']):\n",
    "            for k in range(hyperparams['skip_frames']):\n",
    "                state2, e_reward_, done, info = env.step(action)\n",
    "                e_reward += e_reward_\n",
    "                if done:\n",
    "                    state1 = reset_env()\n",
    "                    break \n",
    "            state_deque.append(prepare_state(state2))\n",
    "        state2 = torch.stack(list(state_deque), dim=1)\n",
    "        replay.add_memory(state1, action, e_reward, state2)\n",
    "        e_reward = 0\n",
    "        if episode_length > hyperparams['max_episode_length']:\n",
    "            if (info['x_pos'] - last_x_pos) < hyperparams['min_progress']:\n",
    "                done = True\n",
    "            else:\n",
    "                last_x_pos = info['x_pos']\n",
    "        if done:\n",
    "            ep_lengths.append(episode_length)\n",
    "            episode_length = 0\n",
    "            state1 = reset_env()\n",
    "            last_x_pos = info_0['x_pos']\n",
    "        else:\n",
    "            state1 = state2\n",
    "        if len(replay.memory) < batch_size:\n",
    "            continue\n",
    "        forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_explicit = False, gamma=gamma)\n",
    "        loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err, lambda_value=lambda_value, beta=beta)\n",
    "        loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean())\n",
    "        losses.append(loss_list)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return ep_lengths, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_8928\\3355323674.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)\n",
      "100%|██████████| 1000/1000 [01:32<00:00, 10.77it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(all_model_params, lr\u001b[38;5;241m=\u001b[39mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m][j])\n\u001b[0;32m      6\u001b[0m ep_lengths, losses \u001b[38;5;241m=\u001b[39m train(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n\u001b[1;32m----> 7\u001b[0m losses_q \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()]\n\u001b[0;32m      8\u001b[0m losses_f \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mcpu()]\n\u001b[0;32m      9\u001b[0m losses_i \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mcpu()]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "# Finding best hyperparameters:\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        replay = ExperienceReplayMemory(N=1500, batch_size=hyperparams['batch_size'][i])\n",
    "        optimizer = optim.Adam(all_model_params, lr=hyperparams['learning_rate'][j])\n",
    "        ep_lengths, losses = train(epochs=1000, batch_size=hyperparams['batch_size'][i])\n",
    "        losses_q = [x[0].detach() for x in losses]\n",
    "        losses_f = [x[1].detach() for x in losses]\n",
    "        losses_i = [x[2].detach() for x in losses]\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.plot(np.log(losses_q), label='Q loss')\n",
    "        plt.plot(np.log(losses_f), label='Forward loss')\n",
    "        plt.plot(np.log(losses_i), label='Inverse loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('losses_'+str(i)+'_'+str(j)+'.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = ExperienceReplayMemory(N=1500, batch_size=hyperparams['batch_size'])\n",
    "qnet = Qnet()\n",
    "qnet.cuda()\n",
    "encoder = Phi()\n",
    "encoder.cuda()\n",
    "forward_model = Fnet()\n",
    "forward_model.cuda()\n",
    "inverse_model = Gnet()\n",
    "inverse_model.cuda()\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "forward_loss.cuda()\n",
    "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "inverse_loss.cuda()\n",
    "qloss = nn.MSELoss()\n",
    "qloss.cuda()\n",
    "all_model_params = list(qnet.parameters()) + list(encoder.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())\n",
    "optimizer = optim.Adam(all_model_params, lr=hyperparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lambda_value=hyperparams['lambda'], beta=hyperparams['beta'], gamma=hyperparams['gamma'], eps = 0.15):\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    state1.to(device)\n",
    "    eps = eps\n",
    "    losses = []\n",
    "    episode_length = 0\n",
    "    switch_to_eps_greedy = 1000\n",
    "    state_deque = deque(maxlen=hyperparams['frames_per_state'])\n",
    "    env.reset()\n",
    "    _, _, _,info_0 = env.step(0)\n",
    "    env.reset()\n",
    "    last_x_pos = info_0['x_pos']\n",
    "    e_reward = 0\n",
    "    ep_lengths = []\n",
    "    #use_explicit = False\n",
    "    for i in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        episode_length += 1\n",
    "        q_val_pred = qnet(state1.cuda())\n",
    "        if i > switch_to_eps_greedy:\n",
    "            action = int(policy(q_val_pred, eps))\n",
    "        else:\n",
    "            action = int(policy(q_val_pred))\n",
    "        for j in range(hyperparams['action_repeats']):\n",
    "            for k in range(hyperparams['skip_frames']):\n",
    "                state2, e_reward_, done, info = env.step(action)\n",
    "                e_reward += e_reward_\n",
    "                if done:\n",
    "                    state1 = reset_env()\n",
    "                    break \n",
    "            state_deque.append(prepare_state(state2))\n",
    "        state2 = torch.stack(list(state_deque), dim=1)\n",
    "        replay.add_memory(state1, action, e_reward, state2)\n",
    "        e_reward = 0\n",
    "        if episode_length > hyperparams['max_episode_length']:\n",
    "            if (info['x_pos'] - last_x_pos) < hyperparams['min_progress']:\n",
    "                done = True\n",
    "            else:\n",
    "                last_x_pos = info['x_pos']\n",
    "        if done:\n",
    "            ep_lengths.append(episode_length)\n",
    "            episode_length = 0\n",
    "            state1 = reset_env()\n",
    "            last_x_pos = info_0['x_pos']\n",
    "        else:\n",
    "            state1 = state2\n",
    "        if len(replay.memory) < hyperparams['batch_size']:\n",
    "            continue\n",
    "        forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_explicit = False, gamma=gamma)\n",
    "        loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err, lambda_value=lambda_value, beta=beta)\n",
    "        loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean())\n",
    "        losses.append(loss_list)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return ep_lengths, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_17596\\3355323674.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)\n",
      "  3%|▎         | 1339/50000 [02:55<1:51:11,  7.29it/s]"
     ]
    }
   ],
   "source": [
    "ep_len, losses_plot =train(epochs=50000, lambda_value=hyperparams['lambda'], beta=hyperparams['beta'], gamma=hyperparams['gamma'], eps = 0.15)\n",
    "losses_q = [x[0].detach().numpy() for x in losses_plot]\n",
    "losses_f = [x[1].detach().numpy() for x in losses_plot]\n",
    "losses_i = [x[2].detach().numpy() for x in losses_plot]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.log(losses_q), label='Q loss')\n",
    "plt.plot(np.log(losses_f), label='Forward loss')\n",
    "plt.plot(np.log(losses_i), label='Inverse loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalità del thread dopo averla impostata\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "eps = 0.1\n",
    "done = True\n",
    "state_deque = deque(maxlen=hyperparams['frames_per_state'])\n",
    "for step in range(5000):\n",
    "    if done: \n",
    "        env.reset()\n",
    "        state1 = prepare_initial_state(env.render(mode='rgb_array'))\n",
    "    q_val_pred = qnet(state1)\n",
    "    action = int(policy(q_val_pred, eps))\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    state2 = prepare_multi_states(state1, state2)\n",
    "    state1 = state2\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
